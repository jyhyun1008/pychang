
# 어라?

색으로 발음기호 표시를 하고, 새로운 데이터셋을 만들러 떠났었습니다. 사실 제게는 녹음된 파일이 이미 60개가 넘게 쌓여 있었고(...이거 적은 거예요), 이 파일을 일일이 손으로 분석해야 하는 상황이었죠.

그런데 더는 그러기가 싫잖아요, 그래서 목소리에 맞는 타이밍과 음정을 미디 파일로 따서 넣어 주면 시간과 음정 값을 따 주는 프로그램을 따로 만들었습니다. 이 부분은 따로 또 포스팅할게요.

...문제는 지금까지 손으로 분석했던 예전 파일들의 타이밍 데이터가 다 틀렸다는 것이었죠.

# 그래서...

그래서 이걸 전부 다 수정해 주었습니다. 아까 만들었다던 미디 프로그램을 돌려서요. 그렇게 저에게는 이제 '제대로 된' 데이터셋 27개가 남아 있었죠.

그런데 이게 끝이 아닙니다.

# 랜덤 지터링 적용

물론 그렇게 새로운 건 아닙니다. 원본 Pix2Pix 논문에 있던 이야기였던 것 같아요. 원래 Pix2Pix 에서는 '랜덤 지터링'이라는 것을 적용했습니다. 쉽게 말하면 이미지가 왼쪽에 있든 오른쪽에 있든 위에 있든 아래에 있든 똑같이 데이터를 뽑아주는 거라고 생각하면 되어요.

![image](https://github.com/jyhyun1008/seodangcat/assets/93899740/eae16a9b-d2f0-49fa-9bc1-4813a2959efb)

하지만 저는 지금껏 이 처리를 안 해왔습니다. 왜냐면 음성 데이터는 미세한 차이로도 결과물의 퀄리티가 엄청 달라질 수 있기 때문이었어요.

그런데 어차피 음성 데이터는 STFT로 변환된 상태. 상하로만 흔들거나 위아래좌우로 뒤집지만 않으면(그러니까 양옆으로만 변화를 주면) 긍정적인 효과를 볼 수 있지 않을까? 싶었습니다.

그래서 냉큼 코드를 추가했고, 이 상태로 36000steps (=1333 epoch) 정도 돌린 음성은 다음과 같이 나오게 됩니다.

<iframe class="youtube" width="560" height="315" src="https://www.youtube.com/embed/yxm8It_96dk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

어라? 이제 발음이라는 게 생겼어요.. 이 상태로 데이터셋만 ~200개 정도로 추가하고, step을 길게 줘서 돌려 보기로 했습니다. (그리고 데이터셋 쪽이 한참 걸릴 것 같네요...)
